## Comparing LLMs

- OpenAI - gpt-4o-mini: 
  - **Pros**: 
    - Fast
    - Cheap
    - Good for simple tasks
  - **Cons**: 
    - Not as powerful as other LLMs
    - Limited in scope

- Anthropic - Claude 3-7 Sonnet
  - **Pros**: 
    - Good for complex tasks
    - More powerful than gpt-4o-mini
  - **Cons**: 
    - Slower
    - More expensive

- Google - Gemini-2.0-flash
  - **Pros**: 
    - Currently Free 
    - Fast
  - **Cons**: 
    - Not as powerful as other LLMs
    - Limited in scope

- Deepsek AI - Deepsek R1
  - **Pros**: 
    - Good for complex tasks
    - More powerful than other LLMs
  - **Cons**:
    - Slower
    - More expensive

- Meta - Llama3
  - **Pros**: 
    - Open-source
    - Good for complex tasks
  - **Cons**: 
    - Slower
    - More expensive

Most of these involve a paid API, with varying costs across each company. Models may alternatively be executed trough LLM Providers. 

- ***Huggingface*** - Huggingface is a platform that hosts open-source AI Models, including LLMs. Some of these models have an enabled Inference API, which allows you to run the model on Huggingface's servers. This is a free service for a limited ammount of usage, and a paid service for larger usage. 

- ***Groq*** - Infraestructure for open-source LLMs. It is a hardware company that provides a platform for running LLMs. It is designed to be easy to use and provides a simple interface for running LLMs. It supports a variety of LLMs, including Llama2, Mistral, and others. 

- ***Ollama*** - Ollama is a tool that allows you to run LLMs ***locally*** on your machine trough optimized C++ code. It is designed to be easy to use and provides a simple interface for running LLMs. It supports a variety of LLMs, including Llama2, Mistral, and others. 

 


The [Vellum Leaderboard](https://www.vellum.ai/llm-leaderboard) gives a comparison of costs and performance of different LLMs.

## APIs


## LLM Providers

