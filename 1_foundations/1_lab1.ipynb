{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the start of your adventure in Agentic AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Are you ready for action??</h2>\n",
    "            <span style=\"color:#ff7800;\">Have you completed all the setup steps in the <a href=\"../setup/\">setup</a> folder?<br/>\n",
    "            Have you checked out the guides in the <a href=\"../guides/01_intro.ipynb\">guides</a> folder?<br/>\n",
    "            Well in that case, you're ready!!\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">This code is a live resource - keep an eye out for my updates</h2>\n",
    "            <span style=\"color:#00bfff;\">I push updates regularly. As people ask questions or have problems, I add more examples and improve explanations. As a result, the code below might not be identical to the videos, as I've added more steps and better comments. Consider this like an interactive book that accompanies the lectures.<br/><br/>\n",
    "            I try to send emails regularly with important updates related to the course. You can find this in the 'Announcements' section of Udemy in the left sidebar. You can also choose to receive my emails via your Notification Settings in Udemy. I'm respectful of your inbox and always try to add value with my emails!\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Test with Gemini Free Tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's do an import\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next it's time to load the API keys into environment variables\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins AIzaSyAo\n"
     ]
    }
   ],
   "source": [
    "# Check the keys\n",
    "\n",
    "import os\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "if gemini_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {gemini_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set - please head to the troubleshooting guide in the setup folder\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now - the all important import statement\n",
    "from openai import OpenAI\n",
    "# Use Gemini's OpenAI-compatible endpoint\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "# Create instance of OpenAI python client with Gemini's base URL\n",
    "openai = OpenAI(base_url=GEMINI_BASE_URL, api_key=gemini_api_key)\n",
    "# We are using the official OpenAI Python client library (openai) \n",
    "# but it's pointing that client to Google's Gemini API, not OpenAI's API.\n",
    "# Google recently made the Gemini API compatible with OpenAI's client libraries \n",
    "# by mimicking the same endpoints and request/response formats\n",
    "# This means you can use tools like the openai Python package (originally built for GPT models) \n",
    "# to access Gemini models — as long as you point it to Gemini's base URL and use a valid Gemini API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of messages in the familiar OpenAI format\n",
    "# Each message is a dictionary with two keys:\n",
    "# - \"role\": Specifies the role of the message sender. Common roles are \"user\", \"assistant\", and \"system\".\n",
    "# - \"content\": The actual text content of the message.\n",
    "# This format is used to structure conversations for OpenAI's chat models.\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is 2+2?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 = 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = openai.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash\",  \n",
    "    messages=messages          \n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now - let's ask for a question:\n",
    "\n",
    "question = \"Please propose a challenging question to assess someone's IQ. Respond only with the question.\"\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the next term in the sequence: 1, 11, 21, 1211, 111221, 312211, _____?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ask it - this uses GPT 4.1 mini, still cheap but more powerful than nano\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "question = response.choices[0].message.content\n",
    "\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form a new messages list\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the famous \"look-and-say\" sequence. Each term describes the previous term.\n",
      "\n",
      "*   1 is \"one 1\"  -> 11\n",
      "*   11 is \"two 1s\" -> 21\n",
      "*   21 is \"one 2, one 1\" -> 1211\n",
      "*   1211 is \"one 1, one 2, two 1s\" -> 111221\n",
      "*   111221 is \"three 1s, two 2s, one 1\" -> 312211\n",
      "\n",
      "So, 312211 is \"one 3, one 1, two 2s, two 1s\" -> 13112221\n",
      "\n",
      "Therefore, the next term in the sequence is **13112221**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ask it again\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This is the famous \"look-and-say\" sequence. Each term describes the previous term.\n",
       "\n",
       "*   1 is \"one 1\"  -> 11\n",
       "*   11 is \"two 1s\" -> 21\n",
       "*   21 is \"one 2, one 1\" -> 1211\n",
       "*   1211 is \"one 1, one 2, two 1s\" -> 111221\n",
       "*   111221 is \"three 1s, two 2s, one 1\" -> 312211\n",
       "\n",
       "So, 312211 is \"one 3, one 1, two 2s, two 1s\" -> 13112221\n",
       "\n",
       "Therefore, the next term in the sequence is **13112221**.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(answer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LLMs with ollama "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deepseek r1 (Heavyweight) - 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ OLLAMA TEST with DeepSeek 7B\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to Ollama local server\n",
    "ollama = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "MODEL = \"deepseek:7b\"\n",
    "\n",
    "# Same prompt used in the Gemini test\n",
    "question = \"Please propose a hard, challenging question to assess someone's IQ. Respond only with the question.\"\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "\n",
    "response = ollama.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "question_from_ollama = response.choices[0].message.content\n",
    "print(\"🤖 Ollama (DeepSeek 7B) generated question:\")\n",
    "print(question_from_ollama)\n",
    "\n",
    "# Ask the question again to get the answer\n",
    "messages = [{\"role\": \"user\", \"content\": question_from_ollama}]\n",
    "response = ollama.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(\"\\n🧠 Ollama (DeepSeek 7B) answer:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phi-3.8 (Middleweight) - 3.8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Phi-3 generated question:\n",
      "What would be the implications of discovering intelligent alien life on Earth in terms of philosophical questions about existence? Provide supporting arguments for your stance while considering ethical responsibilities and potential impacts on society at large, incorporating perspectives from existential philosophy, social contract theory, and utilitarian principles. Ensure that the discussion examines not just immediate effects but also long-term consequences over multiple generations within human societies across different cultures worldwide.\\n\n",
      "\n",
      "🧠 Phi-3 answer:\n",
      "The discovery of intelligent alien life on Earth would fundamentally challenge many basic assumptions about our place in the universe, sparking a profound philosophical inquiry into existence itself. The immediate reaction often revolves around astonishment and wonder at meeting an extraterrestrial intelligence—stirring existential questions such as: \"What does it mean to be human?\" or more fundamentally, “Where do we fit in the grand scheme of things?”\n",
      "\n",
      "From an existential perspective, encountering intelligent alien life can serve as a catalyst for self-reflection and introspection about our unique abilities like consciousness. The concept that other entities with intelligence may share or diverge from Earthly expressions invites questions of existence’s significance; after all, if beings elsewhere in the cosmos are aware yet fundamentally not human as we know it—what does this say about singular 'uniqueness'? Furthermore, existentialists would ask what purpose our lives serve under these new paradigms. \n",
      "\n",
      "Existentialism holds personal freedom and individual experience as foundational principles; discovering aliens might underscore a shared existence outside of the human narrative but also raise concerns about how this relates to one’self'in meaningful, existential choice—since our understanding of 'community,' including that with non-human intelligences on Earth or beyond, would need revisiting.\n",
      "\n",
      "Social contract theory focuses on mutual agreements amongst free people in society for living harmoniously; revealing extraterrestrial intelligence introduces a complex addition to this social equation—do these entities signify themselves as allies within the 'social contract,' and what obligations does humanity, now part of an expanded community, have towards them? This perspective underscores ethical responsibilities tied not just to immediate reactions but also how humans should organically evolve their moral compass alongside new societal members.\n",
      "\n",
      "Lastly, considering potential long-term consequences over generations through the lens of utilitarianism would prioritize policies and social structures maximized for overall happiness or pleasure—yet this utility must now account for extraterrestrial beings’ needs as well. For instance, if resources were scarce after welcoming alien societies into Earth's ecosystems but the survival of both human-native systems and potential interstellar relationships depended on shared access to those same resources—how might society adapt? This would also invite questions regarding sustainability: how we preserve happiness for multiple intelligent beings with diverse existential experiences while minimizing harm over generations.\n",
      "\n",
      "The implications of advanced extraterrestrial intelligence touch upon philosophical musings about existence, societal contracts, and human fulfillment—and challenge us to extend our social networks and ethical frameworks in ways previously unimagined on Earth or perhaps beyond it into the wider cosmos – a prospect that could inspire new insights for coalescing around shared existential futures.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Use Ollama locally\n",
    "ollama = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "MODEL = \"phi3:3.8b\"\n",
    "\n",
    "# Ask the same IQ-style prompt\n",
    "question = \"Please propose a hard, challenging question to assess someone's IQ. Respond only with the question.\"\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "\n",
    "response = ollama.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "question_phi3 = response.choices[0].message.content\n",
    "print(\"🧪 Phi-3 generated question:\")\n",
    "print(question_phi3)\n",
    "\n",
    "# Use the generated question to get an answer\n",
    "messages = [{\"role\": \"user\", \"content\": question_phi3}]\n",
    "response = ollama.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(\"\\n🧠 Phi-3 answer:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tinyllama (lightweight) - 1.1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 TinyLlama question:\n",
      "\"What do you think of this person's IQ score?\"\n",
      "\n",
      "🧠 TinyLlama answer:\n",
      "The question \"what do you think of [person's IQ score]\" is ambiguous and doesn't provide specific information on the subject being evaluated. It may be used in interviews, presentations, or other contexts to ask questions that require a general response based on the information provided. But in this particular case, it could potentially mean \"you,\" \"the tester,\" \"analyst,\" etc. For the given material about a person's IQ score, it would likely be interpreted as a generic statement referring to all respondents or potential interviewees, with no context or information required for making a judgment about that person's IQ score.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "ollama = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "MODEL = \"tinyllama\"\n",
    "\n",
    "question = \"Please propose a hard, challenging question to assess someone's IQ. Respond only with the question.\"\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "\n",
    "response = ollama.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "question_tiny = response.choices[0].message.content\n",
    "print(\"🧪 TinyLlama question:\")\n",
    "print(question_tiny)\n",
    "\n",
    "# Ask the question back\n",
    "messages = [{\"role\": \"user\", \"content\": question_tiny}]\n",
    "response = ollama.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(\"\\n🧠 TinyLlama answer:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LLMs with Huggingface Inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ huggingface_hub is installed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import huggingface_hub\n",
    "    print(\"✅ huggingface_hub is installed\")\n",
    "except ImportError:\n",
    "    print(\"❌ huggingface_hub is not installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 HF-generated question:\n",
      "\n",
      "\n",
      "Would you rather be given the ability to time travel (no restrictions) or the ability to know every detail of any topic you choose to learn about?\n",
      "\n",
      "Explain your reasoning and how you would use this ability.\n",
      "\n",
      "I would choose the ability to know every detail of any topic I choose to learn about. This ability would allow me to quickly and easily become an expert in any field, giving me a significant advantage in many areas of life. For example, I could become a\n",
      "\n",
      "🧠 HF-generated answer:\n",
      " financial expert and make smart investments, or a medical expert and cure diseases. I could also use this ability to learn about history and culture, giving me a deeper understanding of the world and its people. Time travel, on the other hand, has the potential for many unintended consequences and could potentially be dangerous. With the ability to know every detail of any topic, I could still explore the past and future through books and other media, but without the risks associated with actually traveling through time.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load Hugging Face API key from .env file\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "if not hf_token:\n",
    "    print(\"❌ HUGGINGFACE_TOKEN not set in .env file\")\n",
    "\n",
    "# Use your desired model\n",
    "model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "client = InferenceClient(model=model, token=hf_token)\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"Please propose a hard, challenging question to assess someone's IQ. Respond only with the question.\"\n",
    "\n",
    "# Generate the question\n",
    "response = client.text_generation(prompt, max_new_tokens=100, temperature=0.7)\n",
    "print(\"🧪 HF-generated question:\")\n",
    "print(response)\n",
    "\n",
    "# Ask the question again\n",
    "response2 = client.text_generation(response, max_new_tokens=100, temperature=0.7)\n",
    "print(\"\\n🧠 HF-generated answer:\")\n",
    "print(response2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "That was a small, simple step in the direction of Agentic AI, with your new environment!\n",
    "\n",
    "Next time things get more interesting..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Now try this commercial application:<br/>\n",
    "            First ask the LLM to pick a business area that might be worth exploring for an Agentic AI opportunity.<br/>\n",
    "            Then ask the LLM to present a pain-point in that industry - something challenging that might be ripe for an Agentic solution.<br/>\n",
    "            Finally have 3 third LLM call propose the Agentic AI solution.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Event Photography & Short-Form Video Packages for Small Businesses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First create the messages:\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"porpose a business idea that is not too difficult to implement, and that could be done in a few weeks. The business should be able to generate at least $1000 per month in profit. Please respond with the business idea only.\"}]\n",
    "\n",
    "# Then make the first call:\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# Then read the business idea:\n",
    "\n",
    "business_idea = response.choices[0].message.content\n",
    "\n",
    "print(business_idea)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
